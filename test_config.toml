[model]
size = "small"
training_type = "supervised"

[encoder]
model_name = "sentence-transformers/all-MiniLM-L6-v2"
embedding_dim = 384
max_length = 512
device = "cpu"

[projection] 
activation = "relu"

[hypernetwork]
model_size = "small"
input_dim = 512
lora_rank = 16
dropout = 0.1
activation = "gelu"
target_architecture = "llama"

[lora]
rank = 16
alpha = 32.0
dropout = 0.1
bias = "none"
target_modules = ["q_proj", "v_proj"]

[training]
batch_size = 4
learning_rate = 0.0001
num_epochs = 10
warmup_steps = 100